{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for the new image (using ZSL with CLIP): drone\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# Load pre-trained CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate and save embeddings for the dataset using CLIP\n",
    "def generate_and_save_embeddings_with_clip(image_dir, save_path, class_names):\n",
    "    embeddings = {}\n",
    "    labels = {}  # Dictionary to store the class labels corresponding to embeddings\n",
    "    \n",
    "    # Generate text embeddings for the classes (e.g., 'drone', 'bird')\n",
    "    text_inputs = processor(text=class_names, return_tensors=\"pt\", padding=True)\n",
    "    text_embeddings = model.get_text_features(**text_inputs)\n",
    "    \n",
    "    for label in os.listdir(image_dir):  # Loop through each class folder (e.g., Bird, Drone)\n",
    "        label_dir = os.path.join(image_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize((224, 224))  # Resize image to match CLIP's input size\n",
    "                img_array = np.array(img) / 255.0  # Normalize the image to [0, 1]\n",
    "                img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "                # Extract image embedding using CLIP\n",
    "                image_input = processor(images=img, return_tensors=\"pt\")\n",
    "                image_embedding = model.get_image_features(**image_input)\n",
    "\n",
    "                # Normalize the image embedding\n",
    "                image_embedding = image_embedding / image_embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "                # Store the embedding and label (class) for each image\n",
    "                embeddings[img_name] = image_embedding.detach().numpy().flatten()\n",
    "                labels[img_name] = label  # Store the label (class) for the image\n",
    "\n",
    "    # Save the embeddings and labels to files\n",
    "    np.save(save_path + '_embeddings.npy', embeddings)\n",
    "    np.save(save_path + '_labels6.npy', labels)\n",
    "\n",
    "# Function to load the embeddings and labels from the saved files\n",
    "def load_embeddings_and_labels(file_path):\n",
    "    embeddings = np.load(file_path + '_embeddings.npy', allow_pickle=True).item()\n",
    "    labels = np.load(file_path + '_labels.npy', allow_pickle=True).item()\n",
    "    return embeddings, labels\n",
    "\n",
    "# Function to classify an image using Zero-Shot Learning (ZSL) with CLIP\n",
    "def classify_using_zsl(features, embeddings, labels, threshold=0.7):\n",
    "    similarities = {}\n",
    "    for img_name, embedding in embeddings.items():\n",
    "        similarity = cosine_similarity([features], [embedding])[0][0]\n",
    "        similarities[img_name] = similarity\n",
    "    \n",
    "    # Find the image with the highest similarity score\n",
    "    best_match = max(similarities, key=similarities.get)\n",
    "    max_similarity = similarities[best_match]\n",
    "\n",
    "    # If the similarity score is above the threshold, classify it as the best match; otherwise, return \"Unknown\"\n",
    "    if max_similarity >= threshold:\n",
    "        predicted_label = labels[best_match]\n",
    "    else:\n",
    "        predicted_label = \"Unknown\"  # If similarity is below threshold, return 'Unknown'\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Path to save embeddings (updated)\n",
    "save_path = r'D:\\capstone\\Project\\Clip_image_embeddings'  # Path to save the embeddings and labels file\n",
    "\n",
    "# Define class names (this should match your dataset's classes)\n",
    "class_names = [\"drone\", \"bird\"]  # Extend as per your dataset\n",
    "\n",
    "# Generate and save embeddings for the dataset using CLIP\n",
    "image_dir = r\"D:\\capstone\\imageDataset\\Dataset\"  # Path to your dataset folder\n",
    "generate_and_save_embeddings_with_clip(image_dir, save_path, class_names)\n",
    "\n",
    "# Load the embeddings and labels from the saved file\n",
    "embeddings, labels = load_embeddings_and_labels(save_path)\n",
    "\n",
    "# Now, let's classify a new image\n",
    "img_path = r\"D:\\capstone\\imageDataset\\split_dataset\\train\\drone\\UAVS_618.jpg\"  # Replace with actual image path\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.resize((224, 224))  # Resize image to match CLIP's input size\n",
    "\n",
    "# Extract image features using CLIP\n",
    "image_input = processor(images=img, return_tensors=\"pt\")\n",
    "image_embedding = model.get_image_features(**image_input)\n",
    "\n",
    "# Normalize the image embedding\n",
    "image_embedding = image_embedding / image_embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Classify the image using the embeddings and labels\n",
    "predicted_label = classify_using_zsl(image_embedding.detach().numpy().flatten(), embeddings, labels)\n",
    "\n",
    "print(f\"Predicted label for the new image (using ZSL with CLIP): {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and labels saved to D:\\capstone\\Project\\train_embeddings.npy and D:\\capstone\\Project\\train_labels.npy\n",
      "Accuracy: 1.00\n",
      "Confusion Matrix:\n",
      "[[116   0]\n",
      " [  0  79]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bird       1.00      1.00      1.00        79\n",
      "       drone       1.00      1.00      1.00       116\n",
      "\n",
      "    accuracy                           1.00       195\n",
      "   macro avg       1.00      1.00      1.00       195\n",
      "weighted avg       1.00      1.00      1.00       195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Load pre-trained CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch16\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate and save embeddings for the training dataset\n",
    "def generate_and_save_embeddings(image_dir, embeddings_path, labels_path):\n",
    "    embeddings = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for label in os.listdir(image_dir):  # Loop through each class folder (e.g., 'drone', 'bird')\n",
    "        label_dir = os.path.join(image_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_input = processor(images=img, return_tensors=\"pt\")\n",
    "                img_embedding = model.get_image_features(**img_input)\n",
    "                img_embedding = img_embedding / img_embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "                \n",
    "                # Store embedding and corresponding label\n",
    "                embeddings[img_name] = img_embedding.detach().numpy().flatten()\n",
    "                labels[img_name] = label  # Store class label\n",
    "    \n",
    "    # Save embeddings and labels as .npy files\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    np.save(labels_path, labels)\n",
    "    print(f\"Embeddings and labels saved to {embeddings_path} and {labels_path}\")\n",
    "\n",
    "# Function to load embeddings and labels from saved .npy files\n",
    "def load_embeddings_and_labels(embeddings_path, labels_path):\n",
    "    embeddings = np.load(embeddings_path, allow_pickle=True).item()\n",
    "    labels = np.load(labels_path, allow_pickle=True).item()\n",
    "    return embeddings, labels\n",
    "\n",
    "# Function to classify validation dataset using generated embeddings and labels\n",
    "def classify_and_evaluate(valid_dir, train_embeddings, train_labels, threshold=0.2):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for label in os.listdir(valid_dir):  # Loop through validation class folders\n",
    "        label_dir = os.path.join(valid_dir, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_input = processor(images=img, return_tensors=\"pt\")\n",
    "                img_embedding = model.get_image_features(**img_input)\n",
    "                img_embedding = img_embedding / img_embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "                \n",
    "                similarities = cosine_similarity(\n",
    "                    img_embedding.detach().numpy(), \n",
    "                    np.array(list(train_embeddings.values()))\n",
    "                )\n",
    "                \n",
    "                best_match_idx = np.argmax(similarities)\n",
    "                best_match_similarity = similarities[0][best_match_idx]\n",
    "                \n",
    "                if best_match_similarity >= threshold:\n",
    "                    best_match_image_name = list(train_embeddings.keys())[best_match_idx]\n",
    "                    predicted_label = train_labels[best_match_image_name]\n",
    "                else:\n",
    "                    predicted_label = \"Unknown\"\n",
    "                \n",
    "                y_true.append(label)\n",
    "                y_pred.append(predicted_label)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=list(set(y_true + y_pred)))\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, conf_matrix, report\n",
    "\n",
    "# Paths for train and validation datasets\n",
    "train_dir = r\"D:\\capstone\\imageDataset\\split_dataset\\train\"  # Path to training dataset\n",
    "valid_dir = r\"D:\\capstone\\imageDataset\\split_dataset\\valid\"  # Path to validation dataset\n",
    "\n",
    "# Paths to save embeddings and labels\n",
    "embeddings_path = r\"D:\\capstone\\Project\\train_embeddings.npy\"\n",
    "labels_path = r\"D:\\capstone\\Project\\train_labels.npy\"\n",
    "\n",
    "# Step 1: Generate embeddings and labels for the training dataset\n",
    "generate_and_save_embeddings(train_dir, embeddings_path, labels_path)\n",
    "\n",
    "# Step 2: Load generated embeddings and labels\n",
    "train_embeddings, train_labels = load_embeddings_and_labels(embeddings_path, labels_path)\n",
    "\n",
    "# Step 3: Classify validation dataset and evaluate\n",
    "accuracy, conf_matrix, report = classify_and_evaluate(valid_dir, train_embeddings, train_labels)\n",
    "\n",
    "# Step 4: Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
